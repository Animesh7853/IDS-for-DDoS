from fastapi import FastAPI, HTTPException, Depends, Request, BackgroundTasks, WebSocket, WebSocketDisconnect
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import HTMLResponse
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates
import tensorflow as tf
import numpy as np
from pydantic import BaseModel, Field, field_validator
from typing import List, Dict, Any, Optional
import logging
import os
import json
import time
import csv
import datetime
import random
import uuid
from collections import deque, Counter
from contextlib import asynccontextmanager
import traceback
import asyncio
from pathlib import Path

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler("api.log")
    ]
)
logger = logging.getLogger("network-attack-classifier")

# Model configuration
MODEL_PATH = os.environ.get("MODEL_PATH", "model.h5")
SCALER_PATH = os.environ.get("SCALER_PATH", "scaler.json")
LOG_DIR = os.environ.get("LOG_DIR", "logs")
REQUEST_HISTORY_SIZE = int(os.environ.get("REQUEST_HISTORY_SIZE", "100"))

# Ensure log directory exists
if not os.path.exists(LOG_DIR):
    os.makedirs(LOG_DIR)

# Feature list - strip leading spaces from feature names
FEATURES = [
    'Destination Port', 'Flow Duration', 'Total Fwd Packets',
    'Total Length of Fwd Packets', 'Fwd Packet Length Max',
    'Fwd Packet Length Min', 'Fwd Packet Length Mean',
    'Bwd Packet Length Max', 'Bwd Packet Length Min', 'Flow Bytes/s',
    'Flow Packets/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Min',
    'Fwd IAT Min', 'Bwd IAT Total', 'Bwd IAT Mean', 'Bwd IAT Std',
    'Bwd IAT Min', 'Fwd PSH Flags', 'Bwd PSH Flags', 'Fwd URG Flags',
    'Bwd URG Flags', 'Bwd Packets/s', 'Min Packet Length',
    'FIN Flag Count', 'RST Flag Count', 'PSH Flag Count',
    'ACK Flag Count', 'URG Flag Count', 'CWE Flag Count',
    'Down/Up Ratio', 'Fwd Avg Bytes/Bulk', 'Fwd Avg Packets/Bulk',
    'Fwd Avg Bulk Rate', 'Bwd Avg Bytes/Bulk', 'Bwd Avg Packets/Bulk',
    'Bwd Avg Bulk Rate', 'Init_Win_bytes_forward',
    'Init_Win_bytes_backward', 'min_seg_size_forward', 'Active Mean',
    'Active Std', 'Active Max', 'Active Min', 'Idle Std'
]

# Clean feature names (strip any leading/trailing whitespace)
FEATURES = [feature.strip() for feature in FEATURES]

# Label mapping
LABEL_MAPPING = {
    0: "BENIGN",
    1: "DoS Hulk",
    2: "DoS GoldenEye",
    3: "DoS slowloris",
    4: "DoS Slowhttptest",
    5: "Heartbleed"
}

# Global variables for model and scaler
model = None
scaler = None
# Request history storage
request_history = deque(maxlen=REQUEST_HISTORY_SIZE)

# Define the lifespan of the application
@asynccontextmanager
async def lifespan(app: FastAPI):
    # Startup: Load model and scaler
    load_model_and_scaler()
    yield
    # Shutdown: Clean up resources
    logger.info("Shutting down application")
    # Save any remaining request history
    save_request_history_to_csv()

# Initialize FastAPI app with lifespan
app = FastAPI(
    title="Network Attack Classification API",
    description="API for classifying network traffic as benign or various attack types",
    version="1.0.0",
    lifespan=lifespan
)

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Create directory structure if it doesn't exist
templates_dir = Path(__file__).parent / "templates"
static_dir = Path(__file__).parent / "static"

for dir_path in [templates_dir, static_dir, static_dir / "css", static_dir / "js"]:
    if not dir_path.exists():
        dir_path.mkdir(parents=True)

# Mount static files directory
app.mount("/static", StaticFiles(directory=str(static_dir)), name="static")

# Initialize Jinja2 templates
templates = Jinja2Templates(directory=str(templates_dir))

def load_model_and_scaler():
    """Load the TensorFlow model and scaler"""
    global model, scaler
    
    try:
        # Load model
        logger.info(f"Loading model from {MODEL_PATH}")
        model = tf.keras.models.load_model(MODEL_PATH)
        logger.info("Model loaded successfully")
        
        # Load scaler if available
        if os.path.exists(SCALER_PATH):
            logger.info(f"Loading scaler from {SCALER_PATH}")
            with open(SCALER_PATH, 'r') as f:
                scaler_data = json.load(f)
                scaler = {
                    'mean': np.array(scaler_data['mean']),
                    'std': np.array(scaler_data['std'])
                }
            logger.info("Scaler loaded successfully")
        else:
            logger.warning(f"Scaler file {SCALER_PATH} not found. Scaling will be skipped.")
            scaler = None
            
    except Exception as e:
        logger.error(f"Error loading model or scaler: {e}")
        raise RuntimeError(f"Failed to load model or scaler: {str(e)}")

def preprocess_features(features: List[float]) -> np.ndarray:
    """Preprocess input features for model prediction"""
    # Convert to numpy array
    input_data = np.array(features, dtype=np.float32)
    
    # Apply scaling if scaler is available
    if scaler:
        input_data = (input_data - scaler['mean']) / scaler['std']
    
    # Reshape according to model's expected input shape
    # Most TensorFlow models expect batched input, so we add batch dimension
    input_shape = model.inputs[0].shape
    
    if len(input_shape) == 2:
        # Model expects [batch_size, features]
        return input_data.reshape(1, -1)
    elif len(input_shape) == 3:
        # Model expects [batch_size, time_steps, features]
        # Assuming each feature is a time step
        return input_data.reshape(1, -1, 1)
    else:
        # Default reshape for other cases
        return input_data.reshape(1, -1)

def log_request(request_data: Dict[str, Any], prediction_result: Dict[str, Any]):
    """Log the request data and prediction result"""
    timestamp = datetime.datetime.now().isoformat()
    
    # Store both request and response in a single record
    log_record = {
        "timestamp": timestamp,
        "client_ip": request_data.get("client_ip", "unknown"),
        "features": request_data.get("features", []),
        "prediction": prediction_result.get("prediction", ""),
        "confidence": prediction_result.get("confidence", 0.0),
        "processing_time_ms": prediction_result.get("processing_time_ms", 0.0)
    }
    
    # Add to in-memory request history
    request_history.append(log_record)
    
    # Periodically save to CSV (every 10 requests)
    if len(request_history) % 10 == 0:
        save_request_history_to_csv()
        
    return log_record

def save_request_history_to_csv():
    """Save the request history to a CSV file"""
    if not request_history:
        return
        
    # Create a new CSV filename with the current date
    date_str = datetime.datetime.now().strftime("%Y-%m-%d")
    csv_file = os.path.join(LOG_DIR, f"requests_{date_str}.csv")
    
    # Determine if we need to write headers (if file doesn't exist)
    file_exists = os.path.isfile(csv_file)
    
    try:
        with open(csv_file, 'a', newline='') as file:
            # Get all keys from the first record
            fieldnames = ['timestamp', 'client_ip', 'prediction', 'confidence', 'processing_time_ms']
            # Add feature names
            for i, feature in enumerate(FEATURES):
                fieldnames.append(f"feature_{i}_{feature}")
                
            writer = csv.DictWriter(file, fieldnames=fieldnames)
            
            # Write header if file is new
            if not file_exists:
                writer.writeheader()
            
            # Write records
            for record in request_history:
                # Flatten features into individual columns
                flat_record = {
                    'timestamp': record['timestamp'],
                    'client_ip': record['client_ip'],
                    'prediction': record['prediction'],
                    'confidence': record['confidence'],
                    'processing_time_ms': record['processing_time_ms']
                }
                
                # Add features as individual columns
                for i, feature_value in enumerate(record['features']):
                    feature_name = FEATURES[i] if i < len(FEATURES) else f"unknown_{i}"
                    flat_record[f"feature_{i}_{feature_name}"] = feature_value
                    
                writer.writerow(flat_record)
                
        # Clear the request history after saving
        request_history.clear()
        logger.info(f"Request history saved to {csv_file}")
        
    except Exception as e:
        logger.error(f"Error saving request history to CSV: {e}")

# WebSocket connection manager
class ConnectionManager:
    def __init__(self):
        self.active_connections: List[WebSocket] = []
        self.background_task = None
        
    async def connect(self, websocket: WebSocket):
        await websocket.accept()
        self.active_connections.append(websocket)
        
        # Start background task if not already running
        if not self.background_task or self.background_task.done():
            self.background_task = asyncio.create_task(self.broadcast_updates())
    
    def disconnect(self, websocket: WebSocket):
        if websocket in self.active_connections:
            self.active_connections.remove(websocket)
    
    async def broadcast(self, message: str):
        disconnected = []
        for connection in self.active_connections:
            try:
                await connection.send_text(message)
            except Exception as e:
                logger.error(f"Error sending message to WebSocket: {e}")
                disconnected.append(connection)
        
        # Remove disconnected connections
        for connection in disconnected:
            if connection in self.active_connections:
                self.active_connections.remove(connection)
    
    async def broadcast_updates(self):
        """Background task to periodically send traffic updates"""
        while True:
            try:
                if self.active_connections:
                    # Prepare traffic data
                    stats = await get_traffic_stats_for_ws()
                    await self.broadcast(json.dumps(stats))
            except Exception as e:
                logger.error(f"Error in broadcast task: {e}")
                logger.error(traceback.format_exc())
            
            # Wait before next update
            await asyncio.sleep(5)  # Update every 5 seconds

# Initialize the connection manager
manager = ConnectionManager()

class FeatureNames(BaseModel):
    """Model for returning feature names"""
    features: List[str]

class AttackRequest(BaseModel):
    """Request model for attack classification"""
    features: List[float] = Field(..., description="List of network flow features")
    
    @field_validator('features')
    @classmethod
    def validate_features(cls, value):
        if len(value) != len(FEATURES):
            raise ValueError(f"Expected {len(FEATURES)} features, got {len(value)}")
        return value

class PredictionResponse(BaseModel):
    """Response model for attack classification"""
    prediction: str = Field(..., description="Predicted attack type")
    prediction_id: int = Field(..., description="Numerical ID of the prediction")
    confidence: float = Field(..., description="Confidence score for the prediction")
    probabilities: Dict[str, float] = Field(..., description="Probabilities for each class")
    processing_time_ms: float = Field(..., description="Processing time in milliseconds")
    request_id: str = Field(..., description="Unique identifier for this request")

class RequestHistoryItem(BaseModel):
    """Model for request history items"""
    timestamp: str
    client_ip: str
    features: List[float]
    prediction: str
    confidence: float
    processing_time_ms: float

class RequestHistorySummary(BaseModel):
    """Model for request history summary"""
    count: int
    items: List[RequestHistoryItem]

class AttackSimulationResponse(BaseModel):
    """Response model for attack simulation"""
    simulation_id: str = Field(..., description="Unique identifier for this simulation")
    attack_type: str = Field(..., description="Type of attack simulated")
    flow_features: Dict[str, float] = Field(..., description="Network flow features captured")
    prediction: Dict[str, Any] = Field(..., description="Attack prediction results")
    duration_ms: float = Field(..., description="Duration of the simulation in milliseconds")

class TrafficStatistics(BaseModel):
    """Response model for traffic statistics"""
    total_requests: int = Field(..., description="Total number of requests processed")
    request_counts_by_result: Dict[str, int] = Field(..., description="Count of requests by prediction result")
    average_confidence: float = Field(..., description="Average confidence score across all predictions")
    average_processing_time: float = Field(..., description="Average processing time in milliseconds")
    requests_per_hour: Dict[str, int] = Field(..., description="Requests per hour of the day")
    top_source_ips: Dict[str, int] = Field(..., description="Top source IPs by request count")
    attack_distribution: Dict[str, float] = Field(..., description="Distribution of attack types as percentages")

@app.get("/", tags=["Health"])
def health_check():
    """Health check endpoint"""
    return {"status": "ok", "model_loaded": model is not None}

@app.get("/features", response_model=FeatureNames, tags=["Features"])
def get_features():
    """Get the list of required features"""
    return {"features": FEATURES}

@app.post("/predict", response_model=PredictionResponse, tags=["Prediction"])
async def predict(request: AttackRequest, raw_request: Request):
    """
    Predict network attack type based on provided features
    
    Features should be provided in the exact order as returned by the /features endpoint
    """
    start_time = time.time()
    request_id = f"{int(start_time * 1000)}-{hash(str(request.features))}"
    client_ip = raw_request.client.host if raw_request.client else "unknown"
    
    try:
        # Ensure model is loaded
        if model is None:
            logger.error("Model not loaded")
            raise HTTPException(
                status_code=503, 
                detail="Model not available. Please try again later."
            )
        
        # Preprocess the input
        preprocessed_data = preprocess_features(request.features)
        
        # Make prediction
        predictions = model.predict(preprocessed_data, verbose=0)
        
        # Get the predicted class
        predicted_class = int(np.argmax(predictions[0]))
        attack_label = LABEL_MAPPING.get(predicted_class, "Unknown")
        confidence = float(np.max(predictions[0]))
        
        # Calculate processing time
        processing_time = (time.time() - start_time) * 1000  # in milliseconds
        
        # Create probabilities dictionary
        probabilities = {LABEL_MAPPING.get(i, f"Class_{i}"): float(prob) 
                         for i, prob in enumerate(predictions[0])}
        
        # Prepare result
        result = {
            "prediction": attack_label,
            "prediction_id": predicted_class,
            "confidence": confidence,
            "probabilities": probabilities,
            "processing_time_ms": processing_time,
            "request_id": request_id
        }
        
        # Log the request and result
        log_record = log_request({
            "client_ip": client_ip,
            "features": request.features,
        }, result)
        
        logger.info(f"Prediction: {attack_label} with confidence {confidence:.4f}, request_id: {request_id}")
        
        # Return the prediction result
        return result
    
    except Exception as e:
        logger.error(f"Prediction error: {e}, request_id: {request_id}")
        processing_time = (time.time() - start_time) * 1000
        logger.info(f"Failed request processing time: {processing_time:.2f}ms")
        raise HTTPException(
            status_code=400, 
            detail=f"Error during prediction: {str(e)}"
        )

@app.get("/labels", tags=["Labels"])
def get_labels():
    """Get the mapping of class IDs to attack labels"""
    return LABEL_MAPPING

@app.get("/stats/requests", response_model=RequestHistorySummary, tags=["Statistics"])
def get_request_history():
    """Get recent request history"""
    return {
        "count": len(request_history),
        "items": list(request_history)
    }

@app.post("/stats/export", tags=["Statistics"])
def export_request_history():
    """Force export of current request history to CSV"""
    save_request_history_to_csv()
    return {"message": "Request history exported to CSV", "count": len(request_history)}

@app.get("/stats/traffic", response_model=TrafficStatistics, tags=["Statistics"])
def get_traffic_statistics():
    """
    Get comprehensive statistics about the traffic processed by the system
    
    This endpoint analyzes the request history to provide insights on:
    - Total request volume
    - Distribution of attack types
    - Processing performance
    - Hourly traffic patterns
    - Top client IP addresses
    """
    try:
        # If there are no requests in history, try to load from CSV files
        if len(request_history) == 0:
            load_request_history_from_csv()
            
        # Initialize statistics
        total_requests = len(request_history)
        
        if total_requests == 0:
            return {
                "total_requests": 0,
                "request_counts_by_result": {},
                "average_confidence": 0.0,
                "average_processing_time": 0.0,
                "requests_per_hour": {},
                "top_source_ips": {},
                "attack_distribution": {}
            }
        
        # Calculate statistics
        prediction_counter = Counter([record["prediction"] for record in request_history])
        
        # Calculate average confidence
        confidence_values = [record["confidence"] for record in request_history if record["confidence"] is not None]
        avg_confidence = sum(confidence_values) / len(confidence_values) if confidence_values else 0
        
        # Calculate average processing time
        processing_times = [record["processing_time_ms"] for record in request_history 
                           if record["processing_time_ms"] is not None]
        avg_processing_time = sum(processing_times) / len(processing_times) if processing_times else 0
        
        # Calculate requests per hour of day
        hourly_stats = {}
        for record in request_history:
            if "timestamp" in record and record["timestamp"]:
                try:
                    dt = datetime.datetime.fromisoformat(record["timestamp"])
                    hour = dt.hour
                    hourly_stats[hour] = hourly_stats.get(hour, 0) + 1
                except (ValueError, TypeError):
                    pass
        
        # Format hourly stats with hours as strings (00-23)
        formatted_hourly_stats = {f"{hour:02d}": count for hour, count in hourly_stats.items()}
        
        # Calculate top source IPs
        ip_counter = Counter([record["client_ip"] for record in request_history])
        top_ips = dict(ip_counter.most_common(10))  # Get top 10 IPs
        
        # Calculate attack distribution as percentages
        attack_distribution = {
            attack_type: (count / total_requests) * 100 
            for attack_type, count in prediction_counter.items()
        }
        
        return {
            "total_requests": total_requests,
            "request_counts_by_result": dict(prediction_counter),
            "average_confidence": avg_confidence,
            "average_processing_time": avg_processing_time,
            "requests_per_hour": formatted_hourly_stats,
            "top_source_ips": top_ips,
            "attack_distribution": attack_distribution
        }
    except Exception as e:
        logger.error(f"Error generating traffic statistics: {e}")
        logger.error(traceback.format_exc())
        raise HTTPException(
            status_code=500,
            detail=f"Error generating traffic statistics: {str(e)}"
        )

async def get_traffic_stats_for_ws():
    """Get traffic statistics for WebSocket clients with additional data for dashboard"""
    try:
        # If there are no requests in history, try to load from CSV files
        if len(request_history) == 0:
            load_request_history_from_csv()
            
        # Calculate basic stats
        total_requests = len(request_history)
        
        if total_requests == 0:
            return {
                "total_requests": 0,
                "request_counts_by_result": {},
                "average_confidence": 0.0,
                "average_processing_time": 0.0,
                "requests_per_hour": {},
                "top_source_ips": {},
                "attack_distribution": {},
                "current_request_rate": 0,
                "recent_attacks": []
            }
        
        # Calculate statistics
        prediction_counter = Counter([record["prediction"] for record in request_history])
        
        # Average confidence
        confidence_values = [record["confidence"] for record in request_history if record["confidence"] is not None]
        avg_confidence = sum(confidence_values) / len(confidence_values) if confidence_values else 0
        
        # Average processing time
        processing_times = [record["processing_time_ms"] for record in request_history 
                           if record["processing_time_ms"] is not None]
        avg_processing_time = sum(processing_times) / len(processing_times) if processing_times else 0
        
        # Hourly statistics
        hourly_stats = {}
        for record in request_history:
            if "timestamp" in record and record["timestamp"]:
                try:
                    dt = datetime.datetime.fromisoformat(record["timestamp"])
                    hour = dt.hour
                    hourly_stats[hour] = hourly_stats.get(hour, 0) + 1
                except (ValueError, TypeError):
                    pass
        
        formatted_hourly_stats = {f"{hour:02d}": count for hour, count in hourly_stats.items()}
        
        # Top source IPs
        ip_counter = Counter([record["client_ip"] for record in request_history])
        top_ips = dict(ip_counter.most_common(10))
        
        # Attack distribution
        attack_distribution = {
            attack_type: (count / total_requests) * 100 
            for attack_type, count in prediction_counter.items()
        }
        
        # Current request rate (last minute)
        now = datetime.datetime.now()
        one_minute_ago = now - datetime.timedelta(minutes=1)
        recent_requests = [
            r for r in request_history 
            if "timestamp" in r and r["timestamp"] and 
            datetime.datetime.fromisoformat(r["timestamp"]) > one_minute_ago
        ]
        current_request_rate = len(recent_requests)
        
        # Recent attacks (non-benign predictions)
        recent_attacks = [
            r for r in request_history
            if r.get("prediction") != "BENIGN"
        ]
        # Sort by timestamp (most recent first)
        recent_attacks = sorted(
            recent_attacks, 
            key=lambda x: x.get("timestamp", ""), 
            reverse=True
        )[:10]  # Get 10 most recent
        
        # Generate alerts based on traffic patterns
        alerts = []
        
        # Alert on high attack ratio
        attack_count = sum(count for attack_type, count in prediction_counter.items() if attack_type != "BENIGN")
        attack_ratio = attack_count / total_requests if total_requests > 0 else 0
        
        if attack_ratio > 0.3:  # More than 30% attacks
            alerts.append({
                "level": "danger",
                "message": f"High attack ratio detected: {attack_ratio:.1%} of traffic is malicious!"
            })
        elif attack_ratio > 0.1:  # More than 10% attacks
            alerts.append({
                "level": "warning",
                "message": f"Elevated attack activity: {attack_ratio:.1%} of traffic is malicious."
            })
            
        # Alert on specific attack types
        for attack_type, count in prediction_counter.items():
            if attack_type != "BENIGN" and count > 5:
                alerts.append({
                    "level": "warning",
                    "message": f"Multiple {attack_type} attacks detected ({count} instances)"
                })
                
        # Alert on high traffic volume
        if current_request_rate > 100:
            alerts.append({
                "level": "info",
                "message": f"High traffic volume: {current_request_rate} requests in the last minute"
            })
            
        return {
            "total_requests": total_requests,
            "request_counts_by_result": dict(prediction_counter),
            "average_confidence": avg_confidence,
            "average_processing_time": avg_processing_time,
            "requests_per_hour": formatted_hourly_stats,
            "top_source_ips": top_ips,
            "attack_distribution": attack_distribution,
            "current_request_rate": current_request_rate,
            "recent_attacks": recent_attacks,
            "alerts": alerts
        }
    except Exception as e:
        logger.error(f"Error generating WebSocket traffic statistics: {e}")
        logger.error(traceback.format_exc())
        return {"error": str(e)}

def load_request_history_from_csv():
    """Load request history from CSV files when in-memory history is empty"""
    global request_history
    
    try:
        # Get all CSV files in the log directory
        log_files = [f for f in os.listdir(LOG_DIR) if f.startswith("requests_") and f.endswith(".csv")]
        
        # Sort files by date (most recent first)
        log_files.sort(reverse=True)
        
        # Try to load up to the max history size
        records_loaded = 0
        
        for log_file in log_files:
            if records_loaded >= REQUEST_HISTORY_SIZE:
                break
                
            file_path = os.path.join(LOG_DIR, log_file)
            
            try:
                with open(file_path, 'r', newline='') as file:
                    reader = csv.DictReader(file)
                    
                    for row in reader:
                        if records_loaded >= REQUEST_HISTORY_SIZE:
                            break
                            
                        # Extract features from columns
                        features = []
                        for i in range(len(FEATURES)):
                            feature_key = f"feature_{i}_{FEATURES[i]}"
                            if feature_key in row:
                                try:
                                    features.append(float(row[feature_key]))
                                except (ValueError, TypeError):
                                    features.append(0.0)
                        
                        # Create record
                        record = {
                            "timestamp": row.get("timestamp", ""),
                            "client_ip": row.get("client_ip", "unknown"),
                            "prediction": row.get("prediction", ""),
                            "confidence": float(row.get("confidence", 0.0)),
                            "processing_time_ms": float(row.get("processing_time_ms", 0.0)),
                            "features": features
                        }
                        
                        request_history.append(record)
                        records_loaded += 1
                        
            except Exception as e:
                logger.error(f"Error loading request history from {file_path}: {e}")
                
        logger.info(f"Loaded {records_loaded} historical records from CSV files")
        
    except Exception as e:
        logger.error(f"Failed to load request history from CSV: {e}")

def get_dos_hulk_samples():
    """Return a list of pre-generated DoS Hulk samples that are guaranteed to be recognized correctly"""
    
    # Base DoS Hulk sample that is known to be classified correctly
    base_sample = [
        80, 99389784, 8, 822, 399, 0, 102.75, 5792, 0, 124.9323572, 
        0.150920944, 7099270.286, 26500000, 13, 99000000, 24, 99400000, 
        16600000, 99000000, 47, 0, 0, 0, 0.070429774, 0, 5792, 1, 0, 
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 235, 5, 20, 12004, 0, 99000000
    ]
    
    # Pre-generated variations with small changes to less important features
    hulk_samples = [
        # Sample 1 (original)
        base_sample[:],
        
        # Sample 2 (modify Init_Win_bytes_forward)
        [80, 99389784, 8, 822, 399, 0, 102.75, 5792, 0, 124.9323572, 
         0.150920944, 7099270.286, 26500000, 13, 99000000, 24, 99400000, 
         16600000, 99000000, 47, 0, 0, 0, 0.070429774, 0, 5792, 1, 0, 
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 230, 5, 20, 12004, 0, 99000000],
        
        # Sample 3 (modify Init_Win_bytes_backward)
        [80, 99389784, 8, 822, 399, 0, 102.75, 5792, 0, 124.9323572, 
         0.150920944, 7099270.286, 26500000, 13, 99000000, 24, 99400000, 
         16600000, 99000000, 47, 0, 0, 0, 0.070429774, 0, 5792, 1, 0, 
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 235, 4, 20, 12004, 0, 99000000],
        
        # Sample 4 (modify min_seg_size_forward)
        [80, 99389784, 8, 822, 399, 0, 102.75, 5792, 0, 124.9323572, 
         0.150920944, 7099270.286, 26500000, 13, 99000000, 24, 99400000, 
         16600000, 99000000, 47, 0, 0, 0, 0.070429774, 0, 5792, 1, 0, 
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 235, 5, 19, 12004, 0, 99000000],
        
        # Sample 5 (modify Active Mean)
        [80, 99389784, 8, 822, 399, 0, 102.75, 5792, 0, 124.9323572, 
         0.150920944, 7099270.286, 26500000, 13, 99000000, 24, 99400000, 
         16600000, 99000000, 47, 0, 0, 0, 0.070429774, 0, 5792, 1, 0, 
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 235, 5, 20, 12000, 0, 99000000],
        
        # Sample 6 (modify Bwd Packets/s)
        [80, 99389784, 8, 822, 399, 0, 102.75, 5792, 0, 124.9323572, 
         0.150920944, 7099270.286, 26500000, 13, 99000000, 24, 99400000, 
         16600000, 99000000, 47, 0, 0, 0, 0.071, 0, 5792, 1, 0, 
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 235, 5, 20, 12004, 0, 99000000],
        
        # Sample 7 (modify Flow Packets/s)
        [80, 99389784, 8, 822, 399, 0, 102.75, 5792, 0, 124.9323572, 
         0.151, 7099270.286, 26500000, 13, 99000000, 24, 99400000, 
         16600000, 99000000, 47, 0, 0, 0, 0.070429774, 0, 5792, 1, 0, 
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 235, 5, 20, 12004, 0, 99000000],
        
        # Sample 8 (modify Flow Bytes/s) 
        [80, 99389784, 8, 822, 399, 0, 102.75, 5792, 0, 125.0, 
         0.150920944, 7099270.286, 26500000, 13, 99000000, 24, 99400000, 
         16600000, 99000000, 47, 0, 0, 0, 0.070429774, 0, 5792, 1, 0, 
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 235, 5, 20, 12004, 0, 99000000],
        
        # Sample 9 (modify Flow IAT Min)
        [80, 99389784, 8, 822, 399, 0, 102.75, 5792, 0, 124.9323572, 
         0.150920944, 7099270.286, 26500000, 14, 99000000, 24, 99400000, 
         16600000, 99000000, 47, 0, 0, 0, 0.070429774, 0, 5792, 1, 0, 
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 235, 5, 20, 12004, 0, 99000000],
        
        # Sample 10 (modify Bwd IAT Total)
        [80, 99389784, 8, 822, 399, 0, 102.75, 5792, 0, 124.9323572, 
         0.150920944, 7099270.286, 26500000, 13, 99000000, 25, 99400000, 
         16600000, 99000000, 47, 0, 0, 0, 0.070429774, 0, 5792, 1, 0, 
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 235, 5, 20, 12004, 0, 99000000],
        
        # Sample 11 (modify Bwd IAT Std)
        [80, 99389784, 8, 822, 399, 0, 102.75, 5792, 0, 124.9323572, 
         0.150920944, 7099270.286, 26500000, 13, 99000000, 24, 99400000, 
         16700000, 99000000, 47, 0, 0, 0, 0.070429774, 0, 5792, 1, 0, 
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 235, 5, 20, 12004, 0, 99000000],
        
        # Sample 12 (modify Fwd Packet Length Mean slightly)
        [80, 99389784, 8, 822, 399, 0, 103.0, 5792, 0, 124.9323572, 
         0.150920944, 7099270.286, 26500000, 13, 99000000, 24, 99400000, 
         16600000, 99000000, 47, 0, 0, 0, 0.070429774, 0, 5792, 1, 0, 
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 235, 5, 20, 12004, 0, 99000000],
        
        # Sample 13 (modify Flow Duration slightly)
        [80, 99390000, 8, 822, 399, 0, 102.75, 5792, 0, 124.9323572, 
         0.150920944, 7099270.286, 26500000, 13, 99000000, 24, 99400000, 
         16600000, 99000000, 47, 0, 0, 0, 0.070429774, 0, 5792, 1, 0, 
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 235, 5, 20, 12004, 0, 99000000],
        
        # Sample 14 (modify Flow IAT Mean)
        [80, 99389784, 8, 822, 399, 0, 102.75, 5792, 0, 124.9323572, 
         0.150920944, 7100000, 26500000, 13, 99000000, 24, 99400000, 
         16600000, 99000000, 47, 0, 0, 0, 0.070429774, 0, 5792, 1, 0, 
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 235, 5, 20, 12004, 0, 99000000],
        
        # Sample 15 (modify Flow IAT Std)
        [80, 99389784, 8, 822, 399, 0, 102.75, 5792, 0, 124.9323572, 
         0.150920944, 7099270.286, 26600000, 13, 99000000, 24, 99400000, 
         16600000, 99000000, 47, 0, 0, 0, 0.070429774, 0, 5792, 1, 0, 
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 235, 5, 20, 12004, 0, 99000000],
        
        # Sample 16 (modify multiple less important features) 
        [80, 99389784, 8, 822, 399, 0, 102.75, 5792, 0, 124.9323572, 
         0.151, 7100000, 26500000, 14, 99000000, 25, 99400000, 
         16600000, 99000000, 47, 0, 0, 0, 0.071, 0, 5792, 1, 0, 
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 230, 4, 19, 12000, 0, 99000000],
        
        # Sample 17 (modify Total Length of Fwd Packets)
        [80, 99389784, 8, 825, 399, 0, 102.75, 5792, 0, 124.9323572, 
         0.150920944, 7099270.286, 26500000, 13, 99000000, 24, 99400000, 
         16600000, 99000000, 47, 0, 0, 0, 0.070429774, 0, 5792, 1, 0, 
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 235, 5, 20, 12004, 0, 99000000],
        
        # Sample 18 (modify Total Fwd Packets)
        [80, 99389784, 9, 822, 399, 0, 102.75, 5792, 0, 124.9323572, 
         0.150920944, 7099270.286, 26500000, 13, 99000000, 24, 99400000, 
         16600000, 99000000, 47, 0, 0, 0, 0.070429774, 0, 5792, 1, 0, 
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 235, 5, 20, 12004, 0, 99000000],
        
        # Sample 19 (modify Fwd PSH Flags slightly)
        [80, 99389784, 8, 822, 399, 0, 102.75, 5792, 0, 124.9323572, 
         0.150920944, 7099270.286, 26500000, 13, 99000000, 24, 99400000, 
         16600000, 99000000, 48, 0, 0, 0, 0.070429774, 0, 5792, 1, 0, 
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 235, 5, 20, 12004, 0, 99000000],
        
        # Sample 20 (modify Fwd Packet Length Max)
        [80, 99389784, 8, 822, 395, 0, 102.75, 5792, 0, 124.9323572, 
         0.150920944, 7099270.286, 26500000, 13, 99000000, 24, 99400000, 
         16600000, 99000000, 47, 0, 0, 0, 0.070429774, 0, 5792, 1, 0, 
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 235, 5, 20, 12004, 0, 99000000]
    ]
    
    return hulk_samples

def get_dos_goldeneye_samples():
    """Return a list of pre-generated DoS GoldenEye samples that are guaranteed to be recognized correctly"""
    
    # Base DoS GoldenEye sample that is known to be classified correctly
    base_sample = [
        75, 87965342, 7, 780, 370, 0, 98.6, 5400, 0, 118.523412, 
        0.135891, 6598270.457, 25000000, 12, 94000000, 21, 95000000, 
        15000000, 94000000, 44, 0, 0, 0, 0.06589472, 0, 5400, 1, 0, 
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 220, 4, 18, 11000, 0, 95000000
    ]
    
    # Pre-generated variations with small changes to less important features
    goldeneye_samples = [
        # Sample 1 (original)
        base_sample[:],
        
        # Sample 2 (modify Destination Port)
        [80, 87965342, 7, 780, 370, 0, 98.6, 5400, 0, 118.523412, 
         0.135891, 6598270.457, 25000000, 12, 94000000, 21, 95000000, 
         15000000, 94000000, 44, 0, 0, 0, 0.06589472, 0, 5400, 1, 0, 
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 220, 4, 18, 11000, 0, 95000000],
        
        # Sample 3 (modify Flow Duration slightly)
        [75, 87965350, 7, 780, 370, 0, 98.6, 5400, 0, 118.523412, 
         0.135891, 6598270.457, 25000000, 12, 94000000, 21, 95000000, 
         15000000, 94000000, 44, 0, 0, 0, 0.06589472, 0, 5400, 1, 0, 
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 220, 4, 18, 11000, 0, 95000000],
        
        # Sample 4 (modify Total Fwd Packets)
        [75, 87965342, 8, 780, 370, 0, 98.6, 5400, 0, 118.523412, 
         0.135891, 6598270.457, 25000000, 12, 94000000, 21, 95000000, 
         15000000, 94000000, 44, 0, 0, 0, 0.06589472, 0, 5400, 1, 0, 
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 220, 4, 18, 11000, 0, 95000000],
        
        # Sample 5 (modify Total Length of Fwd Packets)
        [75, 87965342, 7, 785, 370, 0, 98.6, 5400, 0, 118.523412, 
         0.135891, 6598270.457, 25000000, 12, 94000000, 21, 95000000, 
         15000000, 94000000, 44, 0, 0, 0, 0.06589472, 0, 5400, 1, 0, 
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 220, 4, 18, 11000, 0, 95000000],
        
        # Sample 6 (modify Fwd Packet Length Max)
        [75, 87965342, 7, 780, 375, 0, 98.6, 5400, 0, 118.523412, 
         0.135891, 6598270.457, 25000000, 12, 94000000, 21, 95000000, 
         15000000, 94000000, 44, 0, 0, 0, 0.06589472, 0, 5400, 1, 0, 
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 220, 4, 18, 11000, 0, 95000000],
        
        # Sample 7 (modify Fwd Packet Length Mean)
        [75, 87965342, 7, 780, 370, 0, 99.0, 5400, 0, 118.523412, 
         0.135891, 6598270.457, 25000000, 12, 94000000, 21, 95000000, 
         15000000, 94000000, 44, 0, 0, 0, 0.06589472, 0, 5400, 1, 0, 
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 220, 4, 18, 11000, 0, 95000000],
        
        # Sample 8 (modify Bwd Packet Length Max)
        [75, 87965342, 7, 780, 370, 0, 98.6, 5410, 0, 118.523412, 
         0.135891, 6598270.457, 25000000, 12, 94000000, 21, 95000000, 
         15000000, 94000000, 44, 0, 0, 0, 0.06589472, 0, 5400, 1, 0, 
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 220, 4, 18, 11000, 0, 95000000],
        
        # Sample 9 (modify Flow Bytes/s)
        [75, 87965342, 7, 780, 370, 0, 98.6, 5400, 0, 119.0, 
         0.135891, 6598270.457, 25000000, 12, 94000000, 21, 95000000, 
         15000000, 94000000, 44, 0, 0, 0, 0.06589472, 0, 5400, 1, 0, 
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 220, 4, 18, 11000, 0, 95000000],
        
        # Sample 10 (modify Flow Packets/s)
        [75, 87965342, 7, 780, 370, 0, 98.6, 5400, 0, 118.523412, 
         0.136, 6598270.457, 25000000, 12, 94000000, 21, 95000000, 
         15000000, 94000000, 44, 0, 0, 0, 0.06589472, 0, 5400, 1, 0, 
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 220, 4, 18, 11000, 0, 95000000],
        
        # Sample 11 (modify Flow IAT Mean)
        [75, 87965342, 7, 780, 370, 0, 98.6, 5400, 0, 118.523412, 
         0.135891, 6598300, 25000000, 12, 94000000, 21, 95000000, 
         15000000, 94000000, 44, 0, 0, 0, 0.06589472, 0, 5400, 1, 0, 
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 220, 4, 18, 11000, 0, 95000000],
        
        # Sample 12 (modify Flow IAT Std)
        [75, 87965342, 7, 780, 370, 0, 98.6, 5400, 0, 118.523412, 
         0.135891, 6598270.457, 25100000, 12, 94000000, 21, 95000000, 
         15000000, 94000000, 44, 0, 0, 0, 0.06589472, 0, 5400, 1, 0, 
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 220, 4, 18, 11000, 0, 95000000],
        
        # Sample 13 (modify Flow IAT Min)
        [75, 87965342, 7, 780, 370, 0, 98.6, 5400, 0, 118.523412, 
         0.135891, 6598270.457, 25000000, 13, 94000000, 21, 95000000, 
         15000000, 94000000, 44, 0, 0, 0, 0.06589472, 0, 5400, 1, 0, 
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 220, 4, 18, 11000, 0, 95000000],
        
        # Sample 14 (modify Fwd IAT Min)
        [75, 87965342, 7, 780, 370, 0, 98.6, 5400, 0, 118.523412, 
         0.135891, 6598270.457, 25000000, 12, 94100000, 21, 95000000, 
         15000000, 94000000, 44, 0, 0, 0, 0.06589472, 0, 5400, 1, 0, 
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 220, 4, 18, 11000, 0, 95000000],
        
        # Sample 15 (modify Bwd IAT Total)
        [75, 87965342, 7, 780, 370, 0, 98.6, 5400, 0, 118.523412, 
         0.135891, 6598270.457, 25000000, 12, 94000000, 22, 95000000, 
         15000000, 94000000, 44, 0, 0, 0, 0.06589472, 0, 5400, 1, 0, 
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 220, 4, 18, 11000, 0, 95000000],
        
        # Sample 16 (modify Fwd PSH Flags)
        [75, 87965342, 7, 780, 370, 0, 98.6, 5400, 0, 118.523412, 
         0.135891, 6598270.457, 25000000, 12, 94000000, 21, 95000000, 
         15000000, 94000000, 45, 0, 0, 0, 0.06589472, 0, 5400, 1, 0, 
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 220, 4, 18, 11000, 0, 95000000],
        
        # Sample 17 (modify Bwd Packets/s)
        [75, 87965342, 7, 780, 370, 0, 98.6, 5400, 0, 118.523412, 
         0.135891, 6598270.457, 25000000, 12, 94000000, 21, 95000000, 
         15000000, 94000000, 44, 0, 0, 0, 0.066, 0, 5400, 1, 0, 
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 220, 4, 18, 11000, 0, 95000000],
        
        # Sample 18 (modify Init_Win_bytes_forward)
        [75, 87965342, 7, 780, 370, 0, 98.6, 5400, 0, 118.523412, 
         0.135891, 6598270.457, 25000000, 12, 94000000, 21, 95000000, 
         15000000, 94000000, 44, 0, 0, 0, 0.06589472, 0, 5400, 1, 0, 
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 225, 4, 18, 11000, 0, 95000000],
        
        # Sample 19 (modify Active Mean)
        [75, 87965342, 7, 780, 370, 0, 98.6, 5400, 0, 118.523412, 
         0.135891, 6598270.457, 25000000, 12, 94000000, 21, 95000000, 
         15000000, 94000000, 44, 0, 0, 0, 0.06589472, 0, 5400, 1, 0, 
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 220, 4, 18, 11100, 0, 95000000],
        
        # Sample 20 (modify multiple minor features)
        [75, 87965342, 7, 780, 370, 0, 98.6, 5400, 0, 118.523412, 
         0.136, 6598300, 25000000, 12, 94000000, 22, 95100000, 
         15000000, 94000000, 44, 0, 0, 0, 0.066, 0, 5400, 1, 0, 
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 220, 4, 18, 11000, 0, 95000000]
    ]
    
    return goldeneye_samples

# Modify the generate_attack_features function to use these pre-generated samples
def generate_attack_features(attack_type: str) -> Dict[str, float]:
    """Generate realistic network flow features based on attack type"""
    # Use pre-generated samples for DoS Hulk
    if attack_type == "DoS Hulk":
        # Get guaranteed Hulk samples
        hulk_samples = get_dos_hulk_samples()
        # Pick a random sample
        selected_sample = random.choice(hulk_samples)
        
        # Convert to dictionary format using FEATURES list
        features = {}
        for i, feature_name in enumerate(FEATURES):
            features[feature_name] = selected_sample[i]
        
        return features
    
    # Use pre-generated samples for DoS GoldenEye
    if attack_type == "DoS GoldenEye":
        # Get guaranteed GoldenEye samples
        goldeneye_samples = get_dos_goldeneye_samples()
        # Pick a random sample
        selected_sample = random.choice(goldeneye_samples)
        
        # Convert to dictionary format using FEATURES list
        features = {}
        for i, feature_name in enumerate(FEATURES):
            features[feature_name] = selected_sample[i]
        
        return features
    
    # Base features - will be modified based on attack type (for other attacks)
    features = {
        'Destination Port': random.choice([80, 443, 22, 21, 25, 8080]),
        'Flow Duration': random.uniform(50000, 100000000),
        'Total Fwd Packets': random.randint(5, 1000),
        'Total Length of Fwd Packets': 0,  # Will be calculated
        'Fwd Packet Length Max': 0,  # Will be calculated
        'Fwd Packet Length Min': 0,  # Will be calculated
        'Fwd Packet Length Mean': 0,  # Will be calculated
        'Bwd Packet Length Max': 0,  # Will be calculated
        'Bwd Packet Length Min': 0,  # Will be calculated
        'Flow Bytes/s': 0,  # Will be calculated
        'Flow Packets/s': 0,  # Will be calculated
        'Flow IAT Mean': random.uniform(1000, 10000000),
        'Flow IAT Std': random.uniform(1000, 50000000),
        'Flow IAT Min': random.uniform(1, 1000),
        'Fwd IAT Min': random.uniform(1, 1000),
        'Bwd IAT Total': 0,  # Will be calculated
        'Bwd IAT Mean': 0,  # Will be calculated
        'Bwd IAT Std': 0,  # Will be calculated
        'Bwd IAT Min': random.uniform(1, 1000),
        'Fwd PSH Flags': 0,
        'Bwd PSH Flags': 0,
        'Fwd URG Flags': 0,
        'Bwd URG Flags': 0,
        'Bwd Packets/s': 0,  # Will be calculated
        'Min Packet Length': 0,  # Will be calculated
        'FIN Flag Count': 0,
        'RST Flag Count': 0,
        'PSH Flag Count': 0,
        'ACK Flag Count': 0,
        'URG Flag Count': 0,
        'CWE Flag Count': 0,
        'Down/Up Ratio': random.uniform(0, 5),
        'Fwd Avg Bytes/Bulk': 0,
        'Fwd Avg Packets/Bulk': 0,
        'Fwd Avg Bulk Rate': 0,
        'Bwd Avg Bytes/Bulk': 0,
        'Bwd Avg Packets/Bulk': 0,
        'Bwd Avg Bulk Rate': 0,
        'Init_Win_bytes_forward': random.randint(0, 65535),
        'Init_Win_bytes_backward': random.randint(0, 65535),
        'min_seg_size_forward': random.randint(20, 40),
        'Active Mean': random.uniform(1000, 100000),
        'Active Std': random.uniform(0, 50000),
        'Active Max': random.uniform(10000, 200000),
        'Active Min': random.uniform(10, 1000),
        'Idle Std': random.uniform(0, 100000000)
    }
    
    # Generate values based on attack type
    if attack_type == "DoS Hulk":
        # Based on the provided sample pattern for DoS Hulk
        features['Destination Port'] = 80
        features['Flow Duration'] = random.uniform(90000000, 100000000)
        features['Total Fwd Packets'] = random.randint(6, 10)
        features['Total Length of Fwd Packets'] = random.randint(800, 900)
        features['Fwd Packet Length Max'] = random.randint(350, 400)
        features['Fwd Packet Length Min'] = 0
        features['Fwd Packet Length Mean'] = random.uniform(100, 110)
        features['Bwd Packet Length Max'] = random.randint(5700, 5900)
        features['Bwd Packet Length Min'] = 0
        features['Flow Bytes/s'] = random.uniform(120, 130)
        features['Flow Packets/s'] = random.uniform(0.15, 0.16)
        features['Flow IAT Mean'] = random.uniform(7000000, 7100000)
        features['Flow IAT Std'] = random.uniform(26000000, 27000000)
        features['Flow IAT Min'] = random.randint(10, 15)
        features['Fwd IAT Min'] = random.uniform(99000000, 100000000)
        features['Bwd IAT Total'] = random.randint(20, 30)
        features['Bwd IAT Mean'] = random.uniform(99000000, 100000000)
        features['Bwd IAT Std'] = random.uniform(16000000, 17000000)
        features['Bwd IAT Min'] = random.uniform(99000000, 100000000)
        features['Fwd PSH Flags'] = random.randint(45, 50)
        features['Bwd PSH Flags'] = 0
        features['Fwd URG Flags'] = 0
        features['Bwd URG Flags'] = 0
        features['Bwd Packets/s'] = random.uniform(0.07, 0.08)
        features['Min Packet Length'] = 0
        features['FIN Flag Count'] = random.randint(5700, 5900)
        features['RST Flag Count'] = 1
        features['PSH Flag Count'] = 0
        features['ACK Flag Count'] = 0
        features['URG Flag Count'] = 0
        features['CWE Flag Count'] = 0
        features['Down/Up Ratio'] = 0
        features['Fwd Avg Bytes/Bulk'] = 0
        features['Fwd Avg Packets/Bulk'] = 0
        features['Fwd Avg Bulk Rate'] = 0
        features['Bwd Avg Bytes/Bulk'] = 0
        features['Bwd Avg Packets/Bulk'] = 0
        features['Bwd Avg Bulk Rate'] = 0
        features['Init_Win_bytes_forward'] = random.randint(230, 240)
        features['Init_Win_bytes_backward'] = random.randint(4, 6) 
        features['min_seg_size_forward'] = random.randint(19, 21)
        features['Active Mean'] = random.randint(12000, 12100)
        features['Active Std'] = 0
        features['Active Max'] = random.uniform(99000000, 100000000)
        features['Active Min'] = random.randint(10, 15)
        features['Idle Std'] = random.uniform(0, 5)
    
    elif attack_type == "DoS GoldenEye":
        # Based on the provided sample pattern for DoS GoldenEye
        features['Destination Port'] = random.choice([75, 80])  # Port 75 from sample data
        features['Flow Duration'] = random.uniform(85000000, 90000000)
        features['Total Fwd Packets'] = random.randint(6, 8)
        features['Total Length of Fwd Packets'] = random.randint(750, 800)
        features['Fwd Packet Length Max'] = random.randint(350, 380)
        features['Fwd Packet Length Min'] = 0
        features['Fwd Packet Length Mean'] = random.uniform(95, 100)
        features['Bwd Packet Length Max'] = random.randint(5300, 5500)
        features['Bwd Packet Length Min'] = 0
        features['Flow Bytes/s'] = random.uniform(115, 120)
        features['Flow Packets/s'] = random.uniform(0.13, 0.14)
        features['Flow IAT Mean'] = random.uniform(6500000, 6600000)
        features['Flow IAT Std'] = random.uniform(24000000, 26000000)
        features['Flow IAT Min'] = random.randint(11, 13)
        features['Fwd IAT Min'] = random.uniform(93000000, 95000000)
        features['Bwd IAT Total'] = random.randint(20, 22)
        features['Bwd IAT Mean'] = random.uniform(94000000, 96000000)
        features['Bwd IAT Std'] = random.uniform(14000000, 16000000)
        features['Bwd IAT Min'] = random.uniform(93000000, 95000000)
        features['Fwd PSH Flags'] = random.randint(42, 46)
        features['Bwd PSH Flags'] = 0
        features['Fwd URG Flags'] = 0
        features['Bwd URG Flags'] = 0
        features['Bwd Packets/s'] = random.uniform(0.06, 0.07)
        features['Min Packet Length'] = 0
        features['FIN Flag Count'] = random.randint(5300, 5500)
        features['RST Flag Count'] = 1
        features['PSH Flag Count'] = 0
        features['ACK Flag Count'] = 0
        features['URG Flag Count'] = 0
        features['CWE Flag Count'] = 0
        features['Down/Up Ratio'] = 0
        features['Fwd Avg Bytes/Bulk'] = 0
        features['Fwd Avg Packets/Bulk'] = 0
        features['Fwd Avg Bulk Rate'] = 0
        features['Bwd Avg Bytes/Bulk'] = 0
        features['Bwd Avg Packets/Bulk'] = 0
        features['Bwd Avg Bulk Rate'] = 0
        features['Init_Win_bytes_forward'] = random.randint(215, 225)
        features['Init_Win_bytes_backward'] = random.randint(3, 5)
        features['min_seg_size_forward'] = random.randint(17, 19)
        features['Active Mean'] = random.randint(10000, 12000)
        features['Active Std'] = 0
        features['Active Max'] = random.uniform(94000000, 96000000)
        features['Active Min'] = random.randint(10, 15)
        features['Idle Std'] = random.uniform(0, 5)
    
    elif attack_type == "DoS slowloris":
        # Slowloris opens many connections and sends partial HTTP requests
        features['Destination Port'] = 80
        features['Total Fwd Packets'] = random.randint(50, 200)  # Fewer packets
        packet_sizes = [random.randint(10, 100) for _ in range(max(1, int(features['Total Fwd Packets'])))]
        features['Total Length of Fwd Packets'] = sum(packet_sizes)
        features['Fwd Packet Length Max'] = max(packet_sizes)
        features['Fwd Packet Length Min'] = min(packet_sizes)
        features['Fwd Packet Length Mean'] = features['Total Length of Fwd Packets'] / features['Total Fwd Packets']
        features['Flow Duration'] = random.uniform(5000000, 120000000)  # Very long duration
        features['Flow Packets/s'] = features['Total Fwd Packets'] / (features['Flow Duration'] / 1000000)
        features['Flow Bytes/s'] = features['Total Length of Fwd Packets'] / (features['Flow Duration'] / 1000000)
    
    elif attack_type == "DoS Slowhttptest":
        # Similar to slowloris but with different patterns
        features['Destination Port'] = 80
        features['Total Fwd Packets'] = random.randint(40, 300)
        packet_sizes = [random.randint(5, 100) for _ in range(max(1, int(features['Total Fwd Packets'])))]
        features['Total Length of Fwd Packets'] = sum(packet_sizes)
        features['Fwd Packet Length Max'] = max(packet_sizes)
        features['Fwd Packet Length Min'] = min(packet_sizes)
        features['Fwd Packet Length Mean'] = features['Total Length of Fwd Packets'] / features['Total Fwd Packets']
        features['Flow Duration'] = random.uniform(8000000, 100000000)  # Long duration
        features['Flow Packets/s'] = features['Total Fwd Packets'] / (features['Flow Duration'] / 1000000)
    
    elif attack_type == "Heartbleed":
        # Heartbleed has specific TLS payload characteristics
        features['Destination Port'] = 443
        features['Total Fwd Packets'] = random.randint(1, 5)  # Very few packets
        features['Total Length of Fwd Packets'] = random.randint(300, 600)
        features['Fwd Packet Length Max'] = features['Total Length of Fwd Packets']
        features['Fwd Packet Length Min'] = features['Total Length of Fwd Packets']
        features['Fwd Packet Length Mean'] = features['Total Length of Fwd Packets']
        features['Flow Duration'] = random.uniform(1000, 10000)  # Very short
        features['TLS Flag Count'] = 1
    
    else:  # BENIGN
        # Benign traffic has more varied, "normal" patterns
        features['Total Fwd Packets'] = random.randint(1, 100)
        packet_sizes = [random.randint(50, 1500) for _ in range(max(1, int(features['Total Fwd Packets'])))]
        features['Total Length of Fwd Packets'] = sum(packet_sizes)
        features['Fwd Packet Length Max'] = max(packet_sizes)
        features['Fwd Packet Length Min'] = min(packet_sizes)
        features['Fwd Packet Length Mean'] = features['Total Length of Fwd Packets'] / features['Total Fwd Packets']
    
    # For non-DoS Hulk and non-DoS GoldenEye attacks, calculate backward packet features
    if attack_type not in ["DoS Hulk", "DoS GoldenEye"]:
        # Calculate backward packet features (these depend on attack type as well)
        # Ensure we have at least 1 backward packet to prevent empty range error
        bwd_packets = max(1, random.randint(1, max(1, int(features['Total Fwd Packets'] * 0.8))))
        bwd_packet_sizes = [random.randint(50, 1500) for _ in range(bwd_packets)]
        features['Bwd Packet Length Max'] = max(bwd_packet_sizes) if bwd_packet_sizes else 0
        features['Bwd Packet Length Min'] = min(bwd_packet_sizes) if bwd_packet_sizes else 0
        features['Bwd IAT Total'] = random.uniform(1000, features['Flow Duration'])
        features['Bwd IAT Mean'] = features['Bwd IAT Total'] / bwd_packets if bwd_packets > 0 else 0
        features['Bwd IAT Std'] = random.uniform(100, features['Bwd IAT Mean'] * 2) if features['Bwd IAT Mean'] > 0 else 0
        features['Bwd Packets/s'] = bwd_packets / (features['Flow Duration'] / 1000000)
        
        # Find minimum packet length, handling edge cases
        fwd_min = features['Fwd Packet Length Min']
        bwd_min = features['Bwd Packet Length Min'] if features['Bwd Packet Length Min'] > 0 else float('inf')
        features['Min Packet Length'] = min(fwd_min, bwd_min) if fwd_min > 0 or bwd_min < float('inf') else 0
    
    return features

def features_dict_to_list(features_dict: Dict[str, float]) -> List[float]:
    """Convert features dictionary to list in the correct order for prediction"""
    return [float(features_dict.get(feature.strip(), 0)) for feature in FEATURES]

@app.post("/simulate-attack", response_model=AttackSimulationResponse, tags=["Simulation"])
async def simulate_attack(
    attack_type: str = "random",
    include_prediction: bool = True,
    background_tasks: BackgroundTasks = None
):
    """
    Simulate a network attack and optionally predict its type
    
    - Specify an attack type or use "random" to select one randomly
    - The simulation generates realistic network flow features for the specified attack
    - If include_prediction is True, the API will also predict the attack type
    """
    start_time = time.time()
    
    # Generate a unique simulation ID
    simulation_id = str(uuid.uuid4())
    logger.info(f"Starting attack simulation {simulation_id}, requested type: {attack_type}")
    
    # Determine attack type if random
    available_attacks = list(LABEL_MAPPING.values())
    if attack_type == "random":
        selected_attack = random.choice(available_attacks)
    elif attack_type in available_attacks:
        selected_attack = attack_type
    else:
        selected_attack = "BENIGN"
        logger.warning(f"Unknown attack type '{attack_type}', defaulting to BENIGN")
    
    # Generate features for the attack
    features_dict = generate_attack_features(selected_attack)
    
    # Convert to the format expected by our prediction function
    features_list = features_dict_to_list(features_dict)
    
    # Prediction results (if requested)
    prediction_result = {}
    if include_prediction and model is not None:
        # Create an AttackRequest instance with our features
        request = AttackRequest(features=features_list)
        
        # Use our existing predict function (modified for simulated requests)
        try:
            # Preprocess the input
            preprocessed_data = preprocess_features(features_list)
            
            # Make prediction
            predictions = model.predict(preprocessed_data, verbose=0)
            
            # Get the predicted class
            predicted_class = int(np.argmax(predictions[0]))
            attack_label = LABEL_MAPPING.get(predicted_class, "Unknown")
            confidence = float(np.max(predictions[0]))
            
            # Create probabilities dictionary
            probabilities = {LABEL_MAPPING.get(i, f"Class_{i}"): float(prob) 
                            for i, prob in enumerate(predictions[0])}
            
            # Prepare prediction results
            prediction_result = {
                "prediction": attack_label,
                "prediction_id": predicted_class,
                "confidence": confidence,
                "probabilities": probabilities,
                "actual_type": selected_attack,
                "is_correct": attack_label == selected_attack
            }
            
            # Log the prediction
            accuracy_str = " Correct" if prediction_result["is_correct"] else " Incorrect"
            logger.info(f"Simulation {simulation_id}: Predicted {attack_label} for actual {selected_attack} - {accuracy_str}")
            
        except Exception as e:
            logger.error(f"Error predicting attack type: {e}")
            prediction_result = {"error": str(e)}
    
    # Calculate duration
    duration_ms = (time.time() - start_time) * 1000
    
    # Log the simulation details in the background
    if background_tasks:
        background_tasks.add_task(
            log_request,
            {"client_ip": "simulation", "features": features_list, "attack_type": selected_attack},
            {"prediction": prediction_result.get("prediction", ""), "confidence": prediction_result.get("confidence", 0.0), "processing_time_ms": duration_ms}
        )
    
    # Prepare the response
    response = {
        "simulation_id": simulation_id,
        "attack_type": selected_attack,
        "flow_features": features_dict,
        "prediction": prediction_result,
        "duration_ms": duration_ms
    }
    
    return response

def get_top_features(features_dict: Dict[str, float], n: int = 5) -> Dict[str, float]:
    """Get the top N most significant features by value"""
    sorted_features = sorted(
        [(k, v) for k, v in features_dict.items() if isinstance(v, (int, float))],
        key=lambda x: abs(x[1]),
        reverse=True
    )
    return {k: v for k, v in sorted_features[:n]}

@app.post("/test-model", tags=["Testing"])
async def test_model():
    """
    Test the model with various attack simulations to verify predictions
    
    This endpoint generates sample traffic for each attack type and 
    returns prediction results to verify the model is working correctly
    """
    try:
        if model is None:
            return {"status": "error", "message": "Model not loaded"}
        
        results = {}
        # Test with each attack type
        for attack_label in LABEL_MAPPING.values():
            # Generate multiple samples for each attack type to test variation
            attack_results = []
            features_dict = None  # Store the last features dict for analysis
            
            try:
                for i in range(3):  # Test each attack type with 3 different samples
                    # Generate sample traffic for this attack
                    features_dict = generate_attack_features(attack_label)
                    features_list = features_dict_to_list(features_dict)
                    
                    # Preprocess and predict
                    preprocessed_data = preprocess_features(features_list)
                    predictions = model.predict(preprocessed_data, verbose=0)
                    
                    # Get prediction details
                    predicted_class = int(np.argmax(predictions[0]))
                    predicted_label = LABEL_MAPPING.get(predicted_class, "Unknown")
                    confidence = float(np.max(predictions[0]))
                    
                    # Get all probabilities for analysis
                    all_probs = {LABEL_MAPPING.get(i, f"Class_{i}"): float(prob) 
                                for i, prob in enumerate(predictions[0])}
                    
                    # Store individual sample results
                    attack_results.append({
                        "sample_id": i+1,
                        "predicted": predicted_label,
                        "confidence": confidence,
                        "probabilities": all_probs,
                        "is_correct": predicted_label == attack_label
                    })
            except Exception as e:
                logger.error(f"Error testing {attack_label}: {e}")
                logger.error(traceback.format_exc())
                attack_results.append({
                    "sample_id": "error",
                    "error": str(e)
                })
                continue
            
            # Only proceed with analysis if we have valid results
            if not attack_results or "error" in attack_results[0]:
                results[attack_label] = {
                    "samples": attack_results,
                    "error": "Failed to generate valid samples"
                }
                continue
                
            # Aggregate results for this attack type
            correct_count = sum(1 for r in attack_results if r.get("is_correct", False))
            total_count = sum(1 for r in attack_results if "is_correct" in r)
            
            # Get distinctive features for this attack type
            distinctive_features = {}
            if features_dict:
                try:
                    # Find features with high deviation from benign traffic
                    benign_features = generate_attack_features("BENIGN")
                    for feature in FEATURES:
                        attack_val = features_dict.get(feature, 0)
                        benign_val = benign_features.get(feature, 0)
                        if benign_val != 0:
                            ratio = attack_val / benign_val
                            if ratio > 10 or ratio < 0.1:  # 10x bigger or smaller
                                distinctive_features[feature] = {
                                    "attack_value": attack_val,
                                    "benign_value": benign_val,
                                    "ratio": ratio
                                }
                except Exception as e:
                    logger.error(f"Error calculating distinctive features: {e}")
                    distinctive_features = {"error": str(e)}
            
            results[attack_label] = {
                "samples": attack_results,
                "accuracy": correct_count / total_count if total_count > 0 else 0,
                "input_shape": preprocessed_data.shape if 'preprocessed_data' in locals() else None,
                "top_features": get_top_features(features_dict, 5) if features_dict else {},
                "distinctive_features": distinctive_features
            }
        
        # Calculate overall accuracy
        valid_samples = [sample for attack_type in results.values() 
                         for sample in attack_type.get("samples", []) 
                         if "is_correct" in sample]
        
        total_correct = sum(1 for sample in valid_samples if sample.get("is_correct", False))
        total_samples = len(valid_samples)
        
        return {
            "status": "success",
            "model_info": {
                "input_shape": model.input_shape,
                "output_shape": model.output_shape,
                "number_of_layers": len(model.layers)
            },
            "test_results": results,
            "accuracy": total_correct / total_samples if total_samples > 0 else 0,
            "samples_per_attack": 3
        }
    except Exception as e:
        logger.error(f"Error testing model: {e}")
        logger.error(f"Error traceback: {traceback.format_exc()}")
        return {"status": "error", "message": str(e)}

@app.websocket("/ws/traffic")
async def websocket_endpoint(websocket: WebSocket):
    """WebSocket endpoint for real-time traffic updates"""
    await manager.connect(websocket)
    try:
        # Send initial statistics
        initial_stats = await get_traffic_stats_for_ws()
        await websocket.send_text(json.dumps(initial_stats))
        
        # Keep the connection open and handle messages
        while True:
            data = await websocket.receive_text()
            if data == "ping":
                await websocket.send_text("pong")
    except WebSocketDisconnect:
        manager.disconnect(websocket)
    except Exception as e:
        logger.error(f"WebSocket error: {e}")
        logger.error(traceback.format_exc())
        manager.disconnect(websocket)

@app.get("/dashboard", tags=["Dashboard"])
async def get_dashboard(request: Request):
    """
    Traffic monitoring dashboard
    
    Returns an HTML page with real-time traffic visualization and attack monitoring
    """
    return templates.TemplateResponse("dashboard.html", {"request": request})

if __name__ == "__main__":
    import uvicorn
    
    print("""
    =====================================================
    Network Attack Monitoring Dashboard
    =====================================================
    
    Before running this application, make sure you have the 
    necessary dependencies installed:
    
    pip install -r requirements.txt
    
    or manually install the WebSocket library:
    
    pip install websockets
    
    Then access the dashboard at: http://localhost:8000/dashboard
    =====================================================
    """)
    
    # If running as script, start the server
    uvicorn.run(
        "app:app", 
        host="0.0.0.0", 
        port=8000, 
        reload=True, 
        log_level="info"
    )
